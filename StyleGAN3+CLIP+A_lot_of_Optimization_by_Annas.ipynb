{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN3+CLIP+A lot of Optimization by Annas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annasajkh/StyleGAN3-CLIP-A-lot-of-Optimization/blob/main/StyleGAN3%2BCLIP%2BA_lot_of_Optimization_by_Annas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizations is from https://arxiv.org/abs/2112.01573 i just modified it a bit\n",
        "#Original method is from https://arxiv.org/abs/2103.17249\n",
        "#Uses StyleGAN2, StyleGAN3, Distil StyleGAN2, Projected GAN, and CLIP you can actually change the generator to any GAN if you want\n",
        "#Created by Annas\n",
        "#twitter @AnnasVirtual"
      ],
      "metadata": {
        "id": "BexInTm04LEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJOrUjaH4J_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b9b303-fd46-4b0c-8411-e1132ce4c43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-fjkmj7ka\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-fjkmj7ka\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.63.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.5)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369221 sha256=6c2c86c7933c29439f251c4b38743b229b49580aeaf400a01d4ddd88574ecaef\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iy0lqtng/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.10.2.3\n",
            "Cloning into 'stylegan3'...\n",
            "remote: Enumerating objects: 193, done.\u001b[K\n",
            "remote: Total 193 (delta 0), reused 0 (delta 0), pack-reused 193\u001b[K\n",
            "Receiving objects: 100% (193/193), 4.18 MiB | 16.83 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "/content/stylegan3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 151MiB/s]\n"
          ]
        }
      ],
      "source": [
        "#<--- just run that don't worry about the code\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install ninja\n",
        "!git clone https://github.com/NVlabs/stylegan3.git\n",
        "%cd stylegan3\n",
        "\n",
        "import torch\n",
        "import clip\n",
        "from torch.optim import AdamW\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from math import sqrt, log\n",
        "import torch.nn.functional as f\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import legacy\n",
        "from typing import Tuple\n",
        "\n",
        "normalize_clip = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "augmentation = nn.Sequential(\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomAffine(degrees=30, translate=(0.0, 0.3)),\n",
        "    transforms.RandomCrop(size=(224, 224))\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "def make_transform(translate: Tuple[float,float], angle: float):\n",
        "    m = np.eye(3)\n",
        "    s = np.sin(angle/360.0*np.pi*2)\n",
        "    c = np.cos(angle/360.0*np.pi*2)\n",
        "    m[0][0] = c\n",
        "    m[0][1] = s\n",
        "    m[0][2] = translate[0]\n",
        "    m[1][0] = -s\n",
        "    m[1][1] = c\n",
        "    m[1][2] = translate[1]\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if you want to change the model just change it here and run the cell\n",
        "#ffhq = generate human faces\n",
        "#afhqv2 = generate animals\n",
        "#afhqcat = generate cats\n",
        "#afhqdog = generate dogs\n",
        "#cifar10 = generate objects\n",
        "#horses  = generate horse\n",
        "#bicycles = generate bicycle\n",
        "#painting = generate painting\n",
        "#landscape = generate landscape\n",
        "#cityscape = generate cityscape\n",
        "\n",
        "#change it here lmao\n",
        "model_name = \"ffhq\"\n",
        "\n",
        "if model_name == \"ffhq\":\n",
        "  !wget \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\" -O \"model.pkl\" #i changed the model to 256x256 cuz it was really slow lol\n",
        "  #you can change it back to stylegan2-ffhq-512x512.pkl if you want \n",
        "elif model_name == \"afhqv2\":\n",
        "  !wget \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-afhqv2-512x512.pkl\" -O \"model.pkl\" #the 512x512 model is so slow so be aware\n",
        "elif model_name == \"afhqcat\":\n",
        "  !wget \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqcat-512x512.pkl\" -O \"model.pkl\" #the 512x512 model is so slow so be aware\n",
        "elif model_name == \"afhqdog\":\n",
        "  !wget \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqdog-512x512.pkl\" -O \"model.pkl\" #the 512x512 model is so slow so be aware\n",
        "elif model_name == \"cifar10\":\n",
        "  !wget \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-cifar10-32x32.pkl\" -O \"model.pkl\" #this one generate really low image quality\n",
        "elif model_name == \"horses\":\n",
        "  !wget \"https://storage.googleapis.com/self-distilled-stylegan/horses_256_pytorch.pkl\" -O \"model.pkl\" #distil stylegan2\n",
        "elif model_name == \"bicycles\":\n",
        "  !wget \"https://storage.googleapis.com/self-distilled-stylegan/bicycles_256_pytorch.pkl\" -O \"model.pkl\" #distil stylegan2\n",
        "elif model_name == \"painting\":\n",
        "  !wget \"https://s3.eu-central-1.amazonaws.com/avg-projects/projected_gan/models/art_painting.pkl\" -O \"model.pkl\" #projected gan\n",
        "elif model_name == \"landscape\":\n",
        "  !wget \"https://s3.eu-central-1.amazonaws.com/avg-projects/projected_gan/models/art_painting.pkl\" -O \"model.pkl\" #projected gan\n",
        "elif model_name == \"cityscape\":\n",
        "  !wget \"https://s3.eu-central-1.amazonaws.com/avg-projects/projected_gan/models/cityscapes.pkl\" -O \"model.pkl\" #projected gan\n",
        "\n",
        "with open(\"model.pkl\", \"rb\")as file:\n",
        "    G = legacy.load_network_pkl(file)[\"G_ema\"].to(device)\n",
        "\n",
        "# Construct an inverse rotation/translation matrix and pass to the generator.  The\n",
        "# generator expects this matrix as an inverse to avoid potentially failing numerical\n",
        "# operations in the network.\n",
        "if hasattr(G.synthesis, \"input\"):\n",
        "  m = make_transform((0, 0), 0)\n",
        "  m = np.linalg.inv(m)\n",
        "  G.synthesis.input.transform.copy_(torch.from_numpy(m))"
      ],
      "metadata": {
        "id": "l6VmK_R34j2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8514af15-e6fc-4de5-f661-50142b711170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-25 08:10:19--  https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\n",
            "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 52.52.30.16, 52.8.78.0\n",
            "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|52.52.30.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 \n",
            "Location: https://prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com/org/nvidia/team/research/models/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl?response-content-disposition=attachment%3B%20filename%3D%22stylegan2-ffhq-256x256.pkl%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMSJGMEQCIBf0Atp72W2eUgYWKWXEBobqOCtpJNBaekHKuhDKL0I9AiAY2mt8CgqGnquUS7QyVSQ8XWLEysbl9LT349oMFUA%2FPyr6AwhxEAMaDDc4OTM2MzEzNTAyNyIMDmNah6uOnHZhvS6qKtcDYUcuIkuWfGCppriPiWiFeVxfRB4PoinR8NZB6Y0%2B%2BqN%2FFkHDBsB%2BN6vlFPSvlLVmicgS8C2kGmVwWZCXI3fwhPxoQJ5MkXM%2BaSsSUtC6fchyL%2BZ%2FRnm0oAWqsDAxi2pkQyVsr3zEkt%2F9kdf%2F2DBqSF17eGZs98hBEEp9DIqSRFgNUCHJMjes5D0BvS%2Bz2UUCmCqYRHBkDP1VmSPSKTH4YNtyHSXKV9vutKa0Ac%2FZXTadfcAIOR5V4Hn%2BU%2B0JyqG53LS0eMP3B4h5QG2KKxJEMVWiPSxZBmm5M6Mr53u%2F7reUG3ElB1LzMAXU5qkLiGOF1rlOtGj2gXit9DFMRb60O%2F9SfdHvVRfmBSm7rWtSG3BkVN4wauhhzAQ1zG4Ne4kbEOEPrZOkNl%2FUmyFskd%2BL%2B9X3Ln0eJokfpRhxie4%2FtXz8fZk8AnQy08Zwi%2BpHdfC4LD8CSX%2BBxyP8MInd13cGOYubDxNfmiNXGe2BJIfGVgtmsdunPUvZKBkNR6CTqVMSLEO3oR1frBSt6RvHPdir7CuYukmJVz584weWG0sFvNby8V4FSRPocO1hF7vhLSrtgzJleNmtHxWKprpth6iQbY6Uh48yfP0oIYGMc9JOFtpRv2F6WxqdMK%2Fs9ZEGOqYB%2FeBXuOjM%2B0g2o4axz2Pn627lpHWuelQuOL4kxiS8b7ld0qqUadf3fHHOmpcYonYegXE3s6PeqGguvzYP9HGi5cj6xfaERnIQ21caD9OWmv341vnMWaWrbm%2BmrRn78zMNyk5mEjz%2FmsgOb6AiJGfdgA%2B3m8Rq5Asf9N0TSC2H4eT5VsKbHQ%2Bd7vguJ98IVBgxk%2F3QLcx4%2BOu1GH6MWwk1xh%2BDWArqSQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220325T081024Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3599&X-Amz-Credential=ASIA3PSNVSIZZHAHYT43%2F20220325%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=12d187dba473f1410dfc933e1a8085fcbf6669b9dbc792a787fcd8e4a11582d2 [following]\n",
            "--2022-03-25 08:10:24--  https://prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com/org/nvidia/team/research/models/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl?response-content-disposition=attachment%3B%20filename%3D%22stylegan2-ffhq-256x256.pkl%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMSJGMEQCIBf0Atp72W2eUgYWKWXEBobqOCtpJNBaekHKuhDKL0I9AiAY2mt8CgqGnquUS7QyVSQ8XWLEysbl9LT349oMFUA%2FPyr6AwhxEAMaDDc4OTM2MzEzNTAyNyIMDmNah6uOnHZhvS6qKtcDYUcuIkuWfGCppriPiWiFeVxfRB4PoinR8NZB6Y0%2B%2BqN%2FFkHDBsB%2BN6vlFPSvlLVmicgS8C2kGmVwWZCXI3fwhPxoQJ5MkXM%2BaSsSUtC6fchyL%2BZ%2FRnm0oAWqsDAxi2pkQyVsr3zEkt%2F9kdf%2F2DBqSF17eGZs98hBEEp9DIqSRFgNUCHJMjes5D0BvS%2Bz2UUCmCqYRHBkDP1VmSPSKTH4YNtyHSXKV9vutKa0Ac%2FZXTadfcAIOR5V4Hn%2BU%2B0JyqG53LS0eMP3B4h5QG2KKxJEMVWiPSxZBmm5M6Mr53u%2F7reUG3ElB1LzMAXU5qkLiGOF1rlOtGj2gXit9DFMRb60O%2F9SfdHvVRfmBSm7rWtSG3BkVN4wauhhzAQ1zG4Ne4kbEOEPrZOkNl%2FUmyFskd%2BL%2B9X3Ln0eJokfpRhxie4%2FtXz8fZk8AnQy08Zwi%2BpHdfC4LD8CSX%2BBxyP8MInd13cGOYubDxNfmiNXGe2BJIfGVgtmsdunPUvZKBkNR6CTqVMSLEO3oR1frBSt6RvHPdir7CuYukmJVz584weWG0sFvNby8V4FSRPocO1hF7vhLSrtgzJleNmtHxWKprpth6iQbY6Uh48yfP0oIYGMc9JOFtpRv2F6WxqdMK%2Fs9ZEGOqYB%2FeBXuOjM%2B0g2o4axz2Pn627lpHWuelQuOL4kxiS8b7ld0qqUadf3fHHOmpcYonYegXE3s6PeqGguvzYP9HGi5cj6xfaERnIQ21caD9OWmv341vnMWaWrbm%2BmrRn78zMNyk5mEjz%2FmsgOb6AiJGfdgA%2B3m8Rq5Asf9N0TSC2H4eT5VsKbHQ%2Bd7vguJ98IVBgxk%2F3QLcx4%2BOu1GH6MWwk1xh%2BDWArqSQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220325T081024Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3599&X-Amz-Credential=ASIA3PSNVSIZZHAHYT43%2F20220325%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=12d187dba473f1410dfc933e1a8085fcbf6669b9dbc792a787fcd8e4a11582d2\n",
            "Resolving prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com (prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com)... 52.218.233.97\n",
            "Connecting to prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com (prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com)|52.218.233.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 295726832 (282M) [application/octet-stream]\n",
            "Saving to: ‘model.pkl’\n",
            "\n",
            "model.pkl           100%[===================>] 282.03M  39.3MB/s    in 7.6s    \n",
            "\n",
            "2022-03-25 08:10:32 (37.2 MB/s) - ‘model.pkl’ saved [295726832/295726832]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"obama smilling with glasses\"\n",
        "\n",
        "lr_init = 0.025\n",
        "\n",
        "steps =  1000 \n",
        "#add more steps if you want better result\n",
        "\n",
        "weight_decay = 0.01\n",
        " \n",
        "image_augmentation_count = 4\n",
        " \n",
        "init_image_augmentation_count = 4\n",
        "\n",
        "init_vectors = 160 * 2 \n",
        "#add more init vectors to get better results though it would hurt the performance \n",
        "\n",
        "init_batch_size = 16\n",
        " \n",
        "show_interval = 50"
      ],
      "metadata": {
        "id": "Ey4BKoJA_33B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#<--- just run that don't worry about the code\n",
        "\n",
        "#RUN THE AI!\n",
        "\n",
        "text_tokenized = clip.tokenize([text]).to(device)\n",
        "label = torch.zeros([init_batch_size, G.c_dim], device=device)\n",
        "\n",
        "losses_latents = []\n",
        "\n",
        "for i in tqdm(range(init_vectors // init_batch_size)):\n",
        "    latents = torch.randn(init_batch_size, G.z_dim).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        images = G(latents, label, truncation_psi=1, noise_mode=\"const\")\n",
        "        images = (images * 127.5 + 128).clamp(0, 255) / 255\n",
        "        images = normalize_clip(images)\n",
        "        images = f.interpolate(images, (224, 224))\n",
        "        \n",
        "        count = 0\n",
        "\n",
        "        for image in images:\n",
        "            images_aug = torch.zeros(init_image_augmentation_count, 3, 224, 224).to(device)\n",
        "\n",
        "            for j in range(init_image_augmentation_count - 1):\n",
        "                images_aug[j] = augmentation(image)\n",
        "\n",
        "            images_aug[-1] = image\n",
        "\n",
        "            texts = torch.repeat_interleave(text_tokenized, init_image_augmentation_count, dim=0)\n",
        "            \n",
        "            losses_latents.append([1 / clip_model(images_aug, texts)[0].mean() * 100, latents[count].unsqueeze(0)])\n",
        "            \n",
        "            count += 1\n",
        "\n",
        "\n",
        "min_loss = min([loss[0] for loss in losses_latents])\n",
        "\n",
        "for loss_latent in losses_latents:\n",
        "    if min_loss == loss_latent[0]:\n",
        "        print(f\"min loss is {loss_latent[0]}\")\n",
        "        latent = loss_latent[1]\n",
        "        break\n",
        "        \n",
        "label = torch.zeros([1, G.c_dim], device=device)\n",
        "latent = nn.Parameter(latent, requires_grad=True)\n",
        "\n",
        "optimizer = AdamW([latent], lr=lr_init, weight_decay=weight_decay)\n",
        "\n",
        "for i in tqdm(range(steps)):\n",
        "    y = G(latent, label, truncation_psi=1, noise_mode=\"const\")\n",
        "    y_norm = (y * 127.5 + 128).clamp(0, 255) / 255\n",
        "    img = normalize_clip(y_norm)\n",
        "    image = f.interpolate(img, (224, 224))\n",
        "    \n",
        "    images = torch.zeros(image_augmentation_count, 3, 224, 224).to(device)\n",
        "    \n",
        "    for j in range(image_augmentation_count - 1):\n",
        "        images[j] = augmentation(image)\n",
        "    \n",
        "    images[-1] = image\n",
        "    \n",
        "    texts = torch.repeat_interleave(text_tokenized, image_augmentation_count, dim=0)\n",
        "    loss = 1 / clip_model(images, texts)[0].mean() * 100\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i % show_interval == 0:\n",
        "        \n",
        "        print(\"\\n\" + text)\n",
        "        print(\"Loss: \" + str(loss.item()))\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            generated = (y.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "            display(Image.fromarray(generated[0].cpu().numpy(), \"RGB\"))\n",
        "    \n",
        "    \n",
        "\n",
        "print(text)\n",
        "print(\"Loss: \" + str(loss.item()))\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated = (y.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "    display(Image.fromarray(generated[0].cpu().numpy(), \"RGB\"))"
      ],
      "metadata": {
        "id": "LMqXp7PqAhtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}